\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell}

%%%%%%%%%%%%%%%%%
% manually added packages
%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{wrapfig}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareUnicodeCharacter{221A}{\ensuremath{\sqrt{}}}
\DeclareMathOperator{\diag}{diag}
\title{Convex Optimization Layers in Neural Architectures: Foundations and Perspectives}
\raggedbottom


\author{%
  Calder K. Katyal\\
  Undergraduate\\
  Yale University\\
  New Haven, CT 06511 \\
  \texttt{calder.katyal@yale.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
    The integration of optimization problems within neural network architectures represents a fundamental shift from traditional approaches to handling constraints in deep learning. While it is long known that neural networks can incorporate soft constraints with techniques such regularization, strict adherence to hard constraints is generally more difficult. A recent advance in this field, however, has addressed this problem by enabling the direct embedding of optimization layers as differentiable components within deep networks. This paper surveys the evolution and current state of this approach, from early implementations limited to quadratic programming to more recent frameworks supporting general convex optimization problems. We provide a comprehensive review of the theoretical foundations, practical implementations, and emerging applications of this technology. Our analysis includes detailed mathematical proofs, implementation guidelines, and an examination of various use cases that demonstrate the potential of this hybrid approach. This work synthesizes developments at the intersection of optimization theory and deep learning, offering insights into both current capabilities and future research directions in this rapidly evolving field. 
\end{abstract}
\section{Introduction}
\label{sec:intro}
Imagine we want to train a model to solve a structured task given a set of input and output sequences, without explicit knowledge of the constraints or rules involved. A typical approach might be to use a feedforward network, such as a Convolutional Neural Network (CNN), to learn patterns from the data. To attempt to learn and conform with the rules, we might try techniques such as regularization or a robust loss function that penalizes rulebreaking. Nevertheless, without a perfectly tuned reward signal from the loss function, the model will tolerate invalid solutions, leading to overfitting as the model may latch onto irrelevant patterns that don’t generalize. If we could incorporate a learnable component into the model that intuitively learns the rules of the task and imposes the output to conform with them, we would expect improved generalization, as the constraints would be independent of specific input distributions and become globally optimized over the training process. However, there is no general mechanism for incorporating complex, learnable hard constraints in a vanilla feedforward network. Only simple constraints—such the simplex constraint in the Softmax layer—are easily enforced. To address this limitation, \citep{optnet} proposes a foundational method of embedding these constrainted optimization problems into the neural architecture as distinct layers. In this layer, the network can output the solution to the constrained optimization problems as part of its forward pass, where the constraints are parametrized by the previous layers; furthermore, it can propagate gradient information through these constraints in the backward pass to better understand the nature of the task. This allows the architecture to enforce either explicitly defined or implicitly learned hard constraints within an end-to-end differentiable framework. 

In this survey, we focus on the development of convex optimization layers as a means to enforce hard constraints within neural network architectures, a capability that has traditionally been challenging to achieve. Starting with the pioneering work of \citep{optnet}, which introduced the embedding of quadratic programming layers, we trace the progression of this idea into more general convex optimization frameworks as in \citep{differentiableconvexoptimizationlayers}. We examine the mathematical principles that enable these layers to remain differentiable, the computational methods required for their efficient implementation, and the integration of these methods into existing neural network workflows. By exploring both theoretical advancements and practical applications, this survey aims to provide a deeper understanding of how convex optimization layers enhance the capacity of neural networks to solve structured problems while adhering to complex constraints. 

\section{Background}

The methods employed in these convex optimization layers are extensions of long-standing theory in the subject of convex optimization, the full treatment of which can be found in \citep{boyd2004convex}. The key to these models is to formulate a learnable optimization problem using the networks parameters and output a solution which can be fed into subsequent layers. The first technique developed for this was an adaptation of \textit{argmin differentiation} in which implicit differentiation is used to obtain the gradients from the Karush-Kuhn-Tucker (KKT) matrix associated with the problem. Subsequent works such as \citep{differentiableconvexoptimizationlayers} transform the problem into a cone problem and use the recently developed technique in \citep{conedifferentiation} to differentiate through the solution map of the cone program. Of course, as QPs are a subset of cone programs, this method is immediately transferable to the optimization layers in \citep{optnet}.

\subsection{Convex Optimization}

A convex optimization layer simply takes the form of a parameterized convex problem, where the output of the layer is the solution \(x^\star\) obtained by minimizing the objective function subject to the constraints.

In particular, we define a (parametrized) convex problem as

\begin{equation}
\begin{aligned}
    \text{minimize} \quad & f_0(x; \theta) \\
    \text{subject to} \quad & f_i(x; \theta) \leq 0, \quad i = 1, \ldots, m, \\
    & a_i^T x = b_i, \quad i = 1, \ldots, p,
\end{aligned}
\end{equation}

where $f_0(x; \theta)$ and $f_i(x; \theta)$, $i = 1, \ldots, m$, are convex functions, and $a_i^T x - b_i$ are affine equality constraints \citep{boyd2004convex}. Here, $x \in \mathbb{R}^n$ is the optimization variable, $\theta \in \mathbb{R}^p$ is a parameter vector, and $a_i \in \mathbb{R}^n$, $b_i \in \mathbb{R}$. The feasible set of this problem is given by \[\mathcal{D} = \bigcap_{i=0}^m \text{dom}(f_i) \cap \bigcap_{i=1}^m \{x \mid f_i(x) \leq 0\} \cap \bigcap_{i=1}^p \{x \mid a_i^T x = b_i\}.
\] and is convex as it is the intersection of convex sets. A convex optimization layer aims to optimize the parameters \(\theta\) such that the solution \(x^\star\), which is the optimal value determined by solving the convex optimization problem parameterized by \(\theta\), minimizes a scalar loss function \(L(x^\star; \theta)\). This ensures that \(\theta\) is learned in a way that aligns the solution \(x^\star\) with the overall task objectives encoded in \(L\), enabling the optimization layer to contribute effectively to the network. Before such a general formulation was considered, the original set of optimization problems as in \citep{optnet} were restricted to convex quadratic programs (QPs) of the form

\begin{equation}
\begin{aligned}
    \text{minimize} \quad & \frac{1}{2} x^T P x + q^T x + r \\
    \text{subject to} \quad & Gx \preceq h, \\
                             & Ax = b,
\end{aligned}
\end{equation}

where \(x \in \mathbb{R}^n\) is the optimization variable, \(P \in \mathbb{S}_+^n\) is a positive semidefinite matrix, \(q \in \mathbb{R}^n\), \(r \in \mathbb{R}\), \(G \in \mathbb{R}^{m \times n}\), \(h \in \mathbb{R}^m\), \(A \in \mathbb{R}^{p \times n}\), and \(b \in \mathbb{R}\), forming a polyhedral feasible set. 

\subsection{Argmin Differentiation}

The method of implicit differentiation used in \citep{optnet} builds of a technique called \textbf{argmin differentiation} that has been previously applied to differentiate through particular optimization problems. The first major treatment of argmin differentiation in a general setting is found in \citep{gould2016differentiating}. While this approach provides theoretical groundwork for differentiating through optimization layers, it treats equality and inequality constraints separately, rather than handling them simultaneously. We briefly review their key results for three cases: unconstrained optimization problems, optimization problems with only equality constraints, and optimization problems with only inequality constraints. The proofs use results from calculus and are omitted here. 

Consider the following general optimization problem: 

\begin{equation}
g(x) = \argmin_{y} f(x,y)
\end{equation}

where $f: \mathbb{R} \times \mathbb{R}^n \rightarrow \mathbb{R}$ is continuous and twice-differentiable. The general results found in \citep{gould2016differentiating} are provided below:

\textbf{Unconstrained} 

The gradient with respect to $x$ can be computed through implicit differentiation:

\begin{equation}
g'(x) = -f_{YY}(x,g(x))^{-1}f_{XY}(x,g(x))
\end{equation}

where $f_{YY} = \nabla^2_{yy}f(x,y)$ and $f_{XY} = \frac{\partial}{\partial x}\nabla_y f(x,y)$.

\textbf{Equality-Constrained}

Suppose we equality constraints of the form $Ay = b$, where $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. For the general case, let $y_0$ be any vector satisfying $Ay_0 = b$ and let $F$ be a matrix whose columns span the null-space of $A$. Then:

\begin{equation}
g'(x) = -F(F^T f_{YY}(x,g(x))F)^{-1}F^T f_{XY}(x,g(x))
\end{equation}

In particular, when $\text{rank}(A) = m$ ($A$ has full row rank), a direct approach yields:
\begin{equation}
g'(x) = (H^{-1}A^T(AH^{-1}A^T)^{-1}AH^{-1} - H^{-1})f_{XY}(x,g(x))
\end{equation}
where $H = f_{YY}(x,g(x))$. 

\textbf{Inequality-Constrained}

Suppose we have inequality constraints $f_i(x,y) \leq 0$, $i=1,\ldots,m$. Using the barrier method (see \citep{boyd2004convex} for a more comprehensive discussion), the constrained problem can be approximated as:

\begin{equation}
\text{minimize}_y \quad tf_0(x,y) - \sum_{i=1}^m \log(-f_i(x,y))
\end{equation}

where $t > 0$ controls the approximation accuracy. The gradient can then be computed as:

\begin{equation}
g'(x) \approx -(tf_{YY}(x,g(x)) + \phi_{YY}(x,g(x)))^{-1}(tf_{XY}(x,g(x)) + \phi_{XY}(x,g(x)))
\end{equation}

where $\phi(x,y) = \sum_{i=1}^m \log(-f_i(x,y))$ is the barrier function.

While providing a mathematical foundation for optimization layers, this approach lacks a unified treatment of mixed constraints. There are also other inefficiencies, such as the use of the barrier method as an approximation to the inequality-constrained case, which can be numerically unstable and computationally expensive. The inversion of the Hessian matrices in the equality-constrained case can furthermore be unsuited for large-scale problems. The more efficient technique of cone differentiation is discussed below. This new approach is shown to be general in \citep{differentiableconvexoptimizationlayers}, as a large subclass of convex optimization problems known as disciplined convex programming (DCP) problems admit reformulations as a cone problems. 

\subsection{Differentiation Through Cone Programs}

While argmin differentiation provides a theoretical foundation for differentiating through optimization problems, a more computationally efficient approach leverages the structure of cone programs. This procedure was developed in \citep{conedifferentiation} and is summarized here. The primal form of a cone program takes the form:
\begin{equation}
\begin{aligned}
\text{minimize} \quad & c^T x \\
\text{subject to} \quad & Ax + s = b \\
                        & s \in K
\end{aligned}
\end{equation}

where $K \subseteq \mathbb{R}^m$ is a proper convex cone (a convex set closed under positive scaling), $x \in \mathbb{R}^n$ is the primal optimization variable, and $s \in \mathbb{R}^m$ is a slack variable that captures inequality constraints. The Lagrangian function for this cone program is given by:
\begin{equation}
\mathcal{L}(x,s,y) = c^T x + y^T(Ax + s - b)
\end{equation}
Therefore, the dual optimization problem is of the form:
\begin{equation}
\begin{aligned}
\text{maximize} \quad & -b^T y \\
\text{subject to} \quad & A^T y + c = 0 \\
                        & y \in K^*
\end{aligned}
\end{equation}

where $y \in \mathbb{R}^m$ represents the dual variables and $K^* = {y \in \mathbb{R}^m : y^T x \geq 0 \text{ for all } x \in K}$ is the dual cone of $K$ (in particular, this is the set of all vectors \( y \) that form non-negative inner products with every vector \( x \) in the cone \( K \)). 

The next step is to understand how optimal solutions depend on and change with problem parameters. To do this systematically, we need to derive the Karush-Kuhn-Tucker conditions for the system. 

\subsubsection{KKT Conditions for Convex Optimization Problems}

The Karush-Kuhn-Tucker (KKT) conditions form a cornerstone of optimization theory, defining optimality criteria for constrained optimization problems. The full treatment can be found in \citep{boyd2004convex}; the main results are summarized below. In the context of convex optimization, these conditions are both necessary and sufficient for optimality. Consider a convex optimization problem where we aim to minimize a convex objective function $f_0(\mathbf{x})$ subject to inequality constraints $f_i(\mathbf{x}) \leq 0$ and equality constraints $h_i(\mathbf{x}) = 0$. The problem can be expressed as:

\begin{align*}
    \text{Minimize } & \quad f_0(\mathbf{x}) \\
    \text{subject to } & \quad f_i(\mathbf{x}) \leq 0, \quad i = 1, \dots, m, \\
    & \quad h_i(\mathbf{x}) = 0, \quad i = 1, \dots, p.
\end{align*}

Here, the \textbf{primal problem} refers to this original formulation, while the \textbf{dual problem} arises by introducing Lagrange multipliers $\lambda_i$ (for $f_i$) and $\nu_i$ (for $h_i$), allowing us to construct a lower bound on the objective function. The dual problem maximizes this bound and provides insights into the sensitivity of the optimal solution to changes in the constraints. The KKT conditions unify these perspectives by characterizing the relationships between the primal and dual problems at optimality. The following theorem is a standard result in convex optimization:

\begin{theorem}[KKT Conditions for Convex Problems]
\label{thm:kkt}
For the above convex optimization problem, where $f_0$ and $f_i$ are convex and $h_i$ are affine, a point $(\mathbf{x}^*, \lambda^*, \nu^*)$ is optimal if and only if the following conditions hold:
\begin{enumerate}
    \item \textbf{Primal feasibility:} $f_i(\mathbf{x}^*) \leq 0$ and $h_i(\mathbf{x}^*) = 0$ for all $i$.
    \item \textbf{Dual feasibility:} $\lambda_i^* \geq 0$ for all $i$.
    \item \textbf{Complementary slackness:} $\lambda_i^* f_i(\mathbf{x}^*) = 0$ for all $i$.
    \item \textbf{Stationarity:}
    \[
    \nabla f_0(\mathbf{x}^*) + \sum_{i=1}^m \lambda_i^* \nabla f_i(\mathbf{x}^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(\mathbf{x}^*) = 0.
    \]
\end{enumerate}
\end{theorem}

Furthermore, as long as there is one strictly feasible point, the primal and dual solutions coincide at optimality via Slater's condition, as strong duality is obtained.

\subsubsection{Solution Map Decomposition}

Thus, in the case of a cone program, we have that:

\begin{enumerate}
    \item \textbf{Primal feasibility} is given by the primal constraints:
    \[
    A x + s = b, \quad s \in K,
    \]

    \item \textbf{Dual feasibility} is given by the dual constraints:
    \[
    A^T y + c = 0, \quad y \in K^*,
    \]
    
    \item \textbf{Complementary slackness} is given by:
    \[
    s^T y = 0,
    \]
    which ensures that the slack variable \( s \) and the dual variable \( y \) are orthogonal, meaning \( s \) and \( y \) cannot both be strictly positive at any point.

    \item The \textbf{stationary} condition is equivalent to the dual feasibility condition, as it is given by:
    \[
    \nabla_x \mathcal{L}(x, s, y) = c + A^T y = 0,
    \]
\end{enumerate}

Thus the solution map to the primal-dual conic program is of the form 
\[
S: \mathbb{R}^{m \times n} \times \mathbb{R}^m \times \mathbb{R}^n \to \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R}^m,
\]
\[
\text{where } S(A, b, c) = (x^*, y^*, s^*) \in 
\left\{
\begin{aligned}
    & (x, y, s) \mid A x + s = b, \; A^T y + c = 0, \\
    & s \in K, \; y \in K^*, \; s^T y = 0
\end{aligned}
\right\}
\]

In particular, we aim to find the derivative of the solution map \(S\) with respect to the problem data \((A, b, c)\), denoted DS(A, b, c). To do this, decompose the solution map \(S: (A,b,c) \mapsto (x,y,s)\) into three components: \(S = \phi \circ \psi \circ Q\), where

\[
Q : \mathbb{R}^{m \times n} \times \mathbb{R}^m \times \mathbb{R}^n \to \mathcal{Q}
\]
\[
s : \mathcal{Q} \to \mathbb{R}^N
\]
\[
\phi : \mathbb{R}^N \to \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R}^m
\]In this formulation, $Q$ maps the problem data to a skew-symmetric matrix (a matrix where \(Q = -Q^T\)) that encodes the problem structure:
\begin{equation}
Q(A,b,c) = \begin{pmatrix} 
0 & A^T & c \\
-A & 0 & b \\
-c^T & -b^T & 0
\end{pmatrix}.
\end{equation}

Furthermore, $s$ yields a solution to the so-called homogeneous self-dual embedding introduced in \citep{Yeselfdualemb} and \citep{Xuselfdualemb}, which a mathematical formulation  that transforms a primal-dual optimization problem into a single system of equations. In particular, solving this system ensures that the resulting solution satisfies all the primal-dual KKT conditions. Finally, $\phi$ maps a solution of the homogeneous self-dual embedding to the solution of the primal-dual pair. 

By the chain rule, we have 

\[
DS(A, b, c) = D\phi(z) \, Ds(Q) \, DQ(A, b, c).
\]

To compute the derivative of the solution map $DS(A,b,c)$, the authors utilize three key transformations ensure mathematical tractability. The first step maps the problem data perturbations $(dA,db,dc)$ into a skew-symmetric matrix $dQ$ by directly arranging them into the corresponding block structure, preserving the critical skew-symmetric property required for feasibility. The most challenging component is computing $Ds$ - this requires solving a large linear system $Mdz = -g$ that encodes how these perturbations affect the solution. Here $M = ((Q-I)D\Pi(z)+I)/w$ combines the problem matrices with projection derivatives, scaled by the solution parameter $w$. The vector $g = dQ\Pi(z/|w|)$ represents how perturbations affect the normalized residual map that measures optimality. Since directly forming and inverting $M$ would be computationally infeasible for large problems, the authors employ LSQR (Least Squares QR) - an iterative method introduced in \citep{lsqr}. LSQR gradually builds up the solution by combining QR matrix factorization with a technique called bidiagonalization, requiring only matrix-vector products rather than storing or inverting the full matrix $M$. The final transformation $D\phi$ converts the computed changes $dz$ into perturbations in the optimization variables $(dx,dy,ds)$ through carefully constructed projections onto the dual cone. For computing derivatives in the backward pass needed for deep learning, this process is reversed using the same efficient numerical techniques. The authors demonstrate this approach scales to problems with millions of variables through an efficient implementation avoiding any explicit matrix operations.

\section{OptNet: Integrating QP Layers Into Neural Architecture}
We begin our discussion of convex optimization layers by deriving the key results in \citep{optnet}, the first major attempt to integrate such layers into neural networks in an end-to-end fashion. The goal is to make the mathematical theory underpinning them rigorous and explore the intuition behind the various components. We will then discuss the expressive power of these layers, and conclude with a discussion of their limitations. 

\subsection{OptNet: Method}

The general idea in \citep{optnet} cast a convex QP as an optimization layer, where the output of the layer $z_{i+1}$ is the solution of the problem and the constraints are parametrized by the output of the previous layer $z_i$. In particular, for optimization variable $z \in \mathbb{R}^n$, we have: 

\[
z_{i+1} = \argmin_{z} \frac{1}{2} z^T Q(z_i) z + q(z_i)^T z
\]
\[
\text{subject to} \quad A(z_i) z = b(z_i), \quad G(z_i) z \leq h(z_i).
\]

where \( Q \in \mathbb{R}^{n \times n} \succeq 0 \) ensures convexity of the objective, and \( q \in \mathbb{R}^n \), 
\( A(z_i) \in \mathbb{R}^{m \times n} \), \( b(z_i) \in \mathbb{R}^m \), \( G(z_i) \in \mathbb{R}^{p \times n} \), and \( h(z_i) \in \mathbb{R}^p \) are problem data. 

The first step is constructing the Lagrangian. For notational clarity, we temporarily suppress the $z_i$ dependence for the rest of the analysis:

\[
L(z,\nu,\lambda) = \frac{1}{2}z^T Q z + q^T z + \nu^T(Az - b) + \lambda^T(Gz - h)
\]

where $\nu \in \mathbb{R}^m, \lambda \in \mathbb{R}^p$ are dual variables and $\lambda \geq 0$. By \ref{thm:kkt}, the optimal solution $(z^*, \nu^*, \lambda^*)$ must satisfy: 

\begin{enumerate}
    \item \textbf{Primal feasibility:} \\
    The primal variables $z^*$ must satisfy the equality and inequality constraints:
    \[
    Az^* = b, \quad Gz^* \leq h.
    \]

    \item \textbf{Dual feasibility:} \\
    As we already noted, the dual variables $\lambda^*$ must be non-negative, because they are associated with inequality constraints:
    \[
    \lambda^* \geq 0.
    \]

    \item \textbf{Complementary slackness:} \\
    For each inequality constraint, either the constraint is active ($Gz^* = h$) or the corresponding dual variable is zero ($\lambda^* = 0$):
    \[
    \lambda_i^* (Gz^* - h)_i = 0 \quad \forall i \in \{1, \ldots, p\}.
    \]

    \item \textbf{Stationarity:} \\
    The gradient of the Lagrangian with respect to the primal variables $z$ must vanish:
    \[
    \nabla_z L(z^*, \nu^*, \lambda^*) = Qz^* + q + A^T\nu^* + G^T\lambda^* = 0.
    \]
\end{enumerate}

Vectorizing the conditions and noting that the stationary condition encompasses the primal feasibility requirement $Gz^* \leq h$, the necessary and sufficient conditions for optimality (given $\lambda \geq 0)$ are: 

\begin{equation}
    \begin{aligned}
        Qz^* + q + A^T\nu^* + G^T\lambda^* = 0 \\
        Az^* - b = 0\\
        \diag({\lambda_i^*})(Gz^* - h)_i = 0
    \end{aligned}
\end{equation}

To approximate how the solution $(z^*, \nu^*, \lambda^*)$ changes with respect to perturbations in the problem parameters, we take the differential:


\begin{equation}
    \begin{aligned}
        dQ z^* + Q dz + dq + dA^T \nu^* + A^T d\nu + dG^T \lambda^* + G^T d\lambda &= 0, \\
        dA z^* + A dz - db &= 0, \\
        \diag(\lambda^*) d(Gz^* - h) + \diag(Gz^* - h) d\lambda &= 0,
    \end{aligned}
\end{equation}

Which admits the matrix formulation:

\begin{equation}
    \begin{bmatrix}
        Q & G^T & A^T \\
        \diag(\lambda^*) G & \diag(Gz^* - h) & 0 \\
        A & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
        dz \\
        d\lambda \\
        d\nu
    \end{bmatrix}
    =
    -\begin{bmatrix}
        dQ z^* + dq + dG^T \lambda^* + dA^T \nu^* \\
        \diag(\lambda^*) dG z^* - \diag(\lambda^*) dh \\
        dA z^* - db
    \end{bmatrix}
\end{equation}

Note that the goal is not to explicitly calculate the Jacobian matrices with respect to the parameters (e.g.$\frac{\partial z^*}{\partial b} \in \mathbb{R}^{n \times m}$), but rather to compute the Jacobian-vector products used in backpropogation (e.g. $\frac{\partial \ell}{\partial z^*} \frac{\partial z^*}{\partial b}$). This avoids explicitly constructing large Jacobian matrices.
To efficiently compute the Jacobian-vector product required in the backward pass, the differential conditions can be leveraged. 

The directional derivatives needed in the chain rule can be found by noting that only $\frac{\partial \ell}{\partial z^*}$ is relevant for propogating gradients and is agnostic to perturbations in the constraints. Therefore, we can write: 

\begin{equation}
\begin{bmatrix}
d_z \\
d_\lambda \\
d_\nu
\end{bmatrix}
=
-
\begin{bmatrix}
Q & G^\top \diag(\lambda^*) & A^\top \\
G & \diag(Gz^* - h) & 0 \\
A & 0 & 0
\end{bmatrix}^{-1}
\begin{bmatrix}
\frac{\partial \ell}{\partial z^*} \\
0 \\
0
\end{bmatrix}.
\end{equation}


The gradient updates are given by: 
\begin{equation}
    \begin{aligned}
        \nabla_Q \ell &= \frac{1}{2}\left(d_z z^{*\top} + z^* d_z^\top\right), \\
        \nabla_A \ell &= d_\nu z^{*\top} + \nu^* d_z^\top, \\
        \nabla_G \ell &= \diag(\lambda^*)d_\lambda z^{*\top} + \lambda^* d_z^\top, \\
        \nabla_q \ell &= d_z, \\
        \nabla_b \ell &= -d_\nu, \\
        \nabla_h \ell &= -\diag(\lambda^*) d_\lambda.
    \end{aligned}
\end{equation}

The authors implement a batched QP solver to make the approach practical for deep learning. Standard solvers like Gurobi and CPLEX, while optimal for solving individual QPs, process QPs sequentially on CPUs. This creates a massive bottleneck when applied to mini-batches in neural networks. To address this, the authors developed a GPU-bound solver that solves all QPs in a mini-batch in parallel, dramatically improving efficiency.

The computational bottleneck lies in factorizing the KKT matrix, which encodes the relationships between the primal and dual variables as well as the constraints. This factorization is necessary because it allows efficient solving of the linear systems required by the primal-dual interior point method. The complexity of factorization is cubic (\( \mathcal{O}(n^3) \)) in the number of variables, but once computed during the forward pass, it is reused to solve linear systems during backpropagation, which has quadratic complexity (\( \mathcal{O}(n^2) \)).

The GPU-based solver exploits parallelism by solving all QPs in the mini-batch simultaneously. For a batch size of \( N \), the parallel implementation reduces the overall complexity to \( \mathcal{O}(n^3 + N n^2) \), compared to the sequential cost of \( \mathcal{O}(N n^3) \) with CPU-based solvers. By sharing the expensive KKT factorization across the batch, the approach ensures both forward and backward passes are computationally efficient and scalable to large mini-batches, making it feasible for end-to-end training in deep learning systems.

 The full details are found in \citep{optnet}, and the solver is based on a primal-dual interior point method developed in \citep{mattingley2012cvxgen}.

\subsection{OptNet: Expressivity}

A key benefit of these layers is their expressive power. The key results obtained in the paper are summarized and proved here:

\begin{theorem}
    
\end{theorem}

\section{Generalization to Convex Programming Layers and Key Adaptations}

We will consider the limitations of the model and how they were addressed in the subsequent major work on the topic \citep{differentiableconvexoptimizationlayers}.
    \subsection{Disciplined Parametrized Programming}
    slka
    \subsection{Affine-Solver-Affine Form}
\subsection{Representational Power}

\section{Implementation}

alsjdlskajd
\subsection{Computational Methods}
    \subsubsection{Interior Point Methods}
    \subsubsection{GPU-Based Solvers}
\subsection{Software Integration}
    \subsubsection{Deep Learning Frameworks}
    \subsubsection{Performance Optimization}
\subsection{Practical Considerations}

\section{Applications}

alkdjlajsd
\subsection{Structured Prediction}
\subsection{Control Systems}
\subsection{Financial Optimization}
\subsection{Machine Learning Models}

\section{Future Directions}
\subsection{Current Limitations}
\subsection{Open Problems}
\subsection{Research Opportunities}

\begin{ack}
Acknowledgments go here.
\end{ack}

\small
\bibliographystyle{plainnat}
\bibliography{references}

% References go here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Appendix / supplemental material}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage



\end{document}
