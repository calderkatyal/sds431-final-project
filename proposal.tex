\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

\PassOptionsToPackage{numbers}{natbib}

% ready for submission
\usepackage[preprint]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024} 

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{calc}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{S\&DS 431 Final Project Proposal}

\author{%
  Calder K. Katyal\\
  Undergraduate\\
  Yale University\\
  New Haven, CT 06511 \\
  \texttt{calder.katyal@yale.edu} \\
}

\begin{document}
\maketitle

Until recently, constraints and optimization in neural networks have been handled indirectly through regularization, custom loss functions, separate solvers, and a variety of other methods that are generally agnostic to neural architecture. In \citet{optnet} researchers propose a model, OptNet, which directly integrates optimization problems as actual layers in deep neural networks. A novel architecture that incorporates quadratic programming (QP) optimization problems as differentiable layers within deep networks, OptNet enables end-to-end training while capturing complex constraints that traditional neural networks struggle to learn. Computational cost is minimized with a GPU-optimized custom solver which uses a primal-dual interior point method to produce backpropagation gradients with minimal overhead. The model exhibits a stunning understanding of hard constraints, learning to play mini-Sudoku from a set of games without even being provided the rules. 

Since the development of OptNet, others have sought to address one of its main challenges: that it assumes a fixed optimization structure and requires users to manually transform problems into a specific form (specifically, quadratic programs). \citet{differentiableconvexoptimizationlayers} develops a framework which allows users to define their optimization problems using a high-level grammar called disciplined parameterized programming (DPP), which extends the usual method of representing convex optimization problems and can be easily implemented in CVXPY. Using this grammar, a wide range of convex optimization problems, not just quadratic programs, can be integrated into different types of differentiable programs. 

The goal of this project is to provide a comprehensive literature review of the integration of convex optimization problems inside neural architecture. We will explore its advantages and limitations, and also examine several use cases. We will also provide basic implementation details to make this method easily accessible to others in the field. Finally, we will rigorously explain and prove the mathematical theorems and intuition that underlie this method. This is a fascinating research frontier in Machine Learning that bridges the fields of optimization and neural networks, and we expect it to continue to grow as more use cases emerge.


\small
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
